<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Macaws Plebs ü¶úüßê</title>

    <link rel="stylesheet" href="../stylesheets/styles.css">
    <link rel="stylesheet" href="../stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
    <title>Aadit Deshpande | My github page</title>
    <meta name="generator" content="Jekyll v3.9.2" />
    <meta property="og:title" content="Aadit Deshpande" />
    <meta name="author" content="Aadit Deshpande" />
    <meta property="og:locale" content="en_US" />
    <meta name="description" content="My github page" />
    <meta property="og:description" content="My github page" />
    <link rel="canonical" href="https://aadit3003.github.io/" />
    <meta property="og:url" content="https://aadit3003.github.io/" />
    <meta property="og:site_name" content="Aadit Deshpande" />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta property="twitter:title" content="Aadit" />
    <script type="application/ld+json">
    {"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Aadit Deshpande"},"description":"My github page","headline":"Aadit Deshpande","name":"Aadit Deshpande","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://aadit3003.github.io/assets/logos/logo.jpg"},"name":"Aadit Deshpande"},"url":"https://aadit3003.github.io/"}</script>
    <!-- End Jekyll SEO tag -->

      <!-- Favicons -->
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>

          <h1><a href="https://aadit3003.github.io/">Aadit Deshpande</a></h1>
        <h4>(he/him)</h4>

        
          <img src="/assets/img/Posts/p1.jpg" alt="DP" />
        

        <p>Contact: <a href="mailto:eylai@mit.edu">f20190077 [at] pilani.bits-pilani.ac.in</a><br>
          GitHub: <a href="https://github.com/aadit3003">aadit3003</a><br>
          Twitter: <a href="https://twitter.com/aadit3003">@Aaditüè≥Ô∏è‚Äçüåà</a><br>
          <!-- Pronouns: he/him <br> -->
          <a href="/assets/docs/Aadit_CV.pdf">CV (PDF)</a>
        </p>


        <p><small> Last Updated: Feb 2023  <br></small>
        </p>

        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
        </ul>
      </header>
      <section>
      </header>
      <section>


      <h1 id="Post 1">Macaws Plebs ü¶úüßê- A PyTorch Mnemonic</h1>

      <p>14 Feb 2023<br />
      <p>I‚Äôve been working on a project involving Split learning, i.e., privacy-preserving decentralized neural network training. Essentially, in situations where we don‚Äôt want to share data (sensitive content like patient records or medical images), we instead just share the activations from multiple client networks to a single server network which then passes back the gradients from backpropagation. I‚Äôll have a more detailed blog post on this in the future when I‚Äôm finished with the project, so keep an eye out for that.</p>

      <p>As a PyTorch beginner, the lack of uniformity in different tutorials regarding the order of operations left me confused. However, I found a really excellent starting point - <a href = "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Deep learning with PyTorch: A 60 min Blitz</a> (I‚Äôd say spend more than 60 min if you want to understand this fully). Inspired by this article, I decided to create a mnemonic <a href="#footnote-1">[1]</a> or acronym to make sure I have all the necessary mise en place to train a neural network module in PyTorch. </p>

      <h2>The Acronym - MCOZ PLBS</h2>


        <table>
          <tbody>
        <tr>
        <th>Letter</th>
        <th>Step</th>
        <th>Description</th>
          </tr>
          <tr>
            <td><b>M</b></td>
        <td>Model</td>
        <td>Instance of the Neural network module class.</td>
          </tr>

          <tr>
            <td><b>C</b></td>
        <td>Criterion</td>
        <td>Pick a loss function (BCE, MSE, etc.).</td>
          </tr>
          
          <tr>
            <td><b>O</b></td>
        <td>Optimizer</td>
        <td>Pick an optimizer (Adam, SGD, etc.).</td>
          </tr>

          <tr>
            <td><b>Z</b></td>
        <td>Zero</td>
        <td>Reset the gradient for each new input.</td>
          </tr>

          <tr>
            <td><b>P</b></td>
        <td>Predict</td>
        <td>Calculate the model‚Äôs prediction on the input.</td>
          </tr>
          
          <tr>
            <td><b>L</b></td>
        <td>Loss</td>

        <td>Calculate the loss (actual vs. predicted label).</td>
          </tr>
          
          <tr>
            <td><b>B</b></td>
        <td>Backward</td>
        <td>Backpropagation.</td>
          </tr>

          <tr>
            <td><b>S</b></td>
        <td>Step</td>
        <td>Parameter update via the optimizer.</td>
          </tr>

        </tbody>
        </table>

      <p>I pronounce it as <em><b>'Macaws Plebs'</b></em> (picture an elitist condescending Macaw ü¶úüßê).</p>

      <h2>Macaws in Action</h2>

<p>Here's how I use MCOZ PLBS to train a typical ResNet classifier. We start by defining some key objects - the model (M), a loss function or criterion (C) and an optimizer (O). (There are a few more steps such as subclassing nn.Module or changing the final FC layer, but I've omitted them for brevity.)</p>


<pre><code>
<b>model = models.resnet34() # M</b>
model.to(device)
<b>criterion = nn.CrossEntropyLoss() # C</b>
<b>optimizer = optim.Adam(model.parameters()) # O</b>
</code></pre>

<p>Now, the training loop to train the model for 50 epochs. We start by resetting the gradient (Z). Then the forward pass (P), loss calculation (L), backward pass (B), and finally, the parameter update (S).</p>

<pre><code>
for epoch in range(50):
  running_loss = 0.0

  for i, data in enumerate(trainloader, 0):
    inputs, labels = data

    <b>optimizer.zero_grad() # Z</b>

    <b>outputs = model(inputs) # P</b>
    <b>loss = criterion(outputs, labels) # L</b>
    <b>loss.backward() # B</b>
    <b>optimizer.step() # S</b>

    running_loss += loss.item()

    if i % 100 == 99:
      print(f"[{epoch}, {i}] loss: {running_loss/100}")
      running_loss = 0.0

print("DONE!!")
</code></pre>



      <h2>A Split Macaw</h2>
<p>'MCOZ PLBS' makes it really easy to implement all kinds of variations of the vanilla training loop. For example, here is a simple Split Learning classification training loop with a client network and a server network. The client side has the data and does a feedforward calculation on the input data upto a 'cutoff layer', after which the server takes over without ever seeing the data. Essentially, we have two of everything - two models (M), a loss function (C), and two optimizers (O). (The code is inspired by this <a href = "https://www.youtube.com/watch?v=VPL6ELWdJbg"> workshop </a> by the Camera Culture group, MIT.)</p>


<pre><code>
<b>client_model = Client({"cut_layer":3}).to(device) # M</b>
<b>server_model = Server({"cut_layer":3}).to(device) # M</b>
criterion = nn.CrossEntropyLoss() # C
<b>client_optimizer = optim.Adam(model.parameters()) # O</b>
<b>server_optimizer = optim.Adam(model.parameters()) # O</b>
</code></pre>


<p> Now, in the training loop we have a division of responsibility - The client handles the forward pass, while the server (data agnostic) performs backpropagation. All this can be simply achieved by doing the steps of MCOZ PLBS twice. We can also scale this to multiple clients and train the network without any of the clients needing to share the data with the server.</p>
<pre><code>
for epoch in range(50):
  running_loss = 0.0

  for i, data in enumerate(trainloader, 0):
    inputs, labels = data

    client_optimizer.zero_grad() # Z
    server_optimizer.zero_grad() # Z

    <b>activations = client_model(inputs) # P</b>
    server_inputs = activations.detach().clone()
    server_inputs = Variable(server_inputs, requires_grad = True)
    <b>outputs = server_model(server_inputs) # P</b>

    loss = criterion(outputs, labels) # L

    <b>loss.backward() # B</b>
    server_optimizer.step() # S
    <b>activations.backward(server_inputs.grad) # B</b>
    client_optimizer.step() # S

    running_loss += loss.item()

    if i % 100 == 99:
      print(f"[{epoch}, {i}] loss: {running_loss/100}")
      running_loss = 0.0

print("DONE!!")
</code></pre>



      <p>And that‚Äôs it! You now have all the ingredients to train your own neural network in PyTorch I‚Äôd love to hear from my readers if you all have fun acronyms or mnemonics for CS-ey stuff.</p>


      <p id="footnote-1"><em>[1] The spelling of the word ‚Äòmnemonic‚Äô has been permanently stamped into my brain since age 11 when I misspelled the word in a National Spelling Bee as ‚ÄúP-N-E-M-O-N-I-C‚Äù  üíÄ. In my defense, pneumonia, and pneumonic sound very similar.</em></p>















      <p><a href="../Blog.html">Back to blog</a></p>


      </section>
<!--       <footer>
        <p>This project is maintained by <a href="http://github.com/orderedlist">Steve Smith</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer> -->
    </div>
    <script src="javascripts/scale.fix.js"></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TSW4QMMRZF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TSW4QMMRZF');
    </script>
  </body>
</html>
